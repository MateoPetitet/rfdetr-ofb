{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0e3d1049",
   "metadata": {},
   "source": [
    "# Fine-tuning : RF-DETR\n",
    "\n",
    "Objectif : faire de RF-DETR un détecteur de poissons sur des images de 1920\\*1080 pixels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4d556102",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device utilisé : cuda\n"
     ]
    }
   ],
   "source": [
    "# Importations\n",
    "import os\n",
    "import numpy as np\n",
    "import torch\n",
    "import torchvision\n",
    "from torchvision import transforms\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import json\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "import pycocotools.coco as coco\n",
    "from PIL import Image\n",
    "from rfdetr import RFDETRLarge, RFDETRBase  # Assurez-vous que le package rf-detr est bien installé\n",
    "\n",
    "\n",
    "# Vérification de la disponibilité du GPU\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Device utilisé :\", device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1b1d07d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = \"/home/mateo/Travail/Fine-tune/Dataset/Roboflow\"  # chemin vers votre jeu de données\n",
    "#images_dir = os.path.join(data_dir, \"luderick-seagrass/train\")\n",
    "#annotations_file = os.path.join(images_dir, \"_annotations.coco.json\")\n",
    "\n",
    "# Charger les annotations COCO\n",
    "#coco_dataset = coco.COCO(annotations_file)\n",
    "#print(\"Nombre d'images dans le dataset :\", len(coco_dataset.imgs))\n"
   ]
  },
  {
   "cell_type": "raw",
   "id": "b0accf12",
   "metadata": {},
   "source": [
    "class CustomCOCODataset(Dataset):\n",
    "    def __init__(self, coco_obj, images_dir):\n",
    "        self.coco = coco_obj\n",
    "        self.images_dir = images_dir\n",
    "        self.image_ids = list(self.coco.imgs.keys())\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.image_ids)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        # Récupérer l'identifiant et les métadonnées de l'image\n",
    "        img_id = self.image_ids[idx]\n",
    "        img_info = self.coco.imgs[img_id]\n",
    "        img_path = os.path.join(self.images_dir, img_info['file_name'])\n",
    "        \n",
    "        # Ouvrir l'image et la convertir en RGB\n",
    "        image = Image.open(img_path).convert(\"RGB\")\n",
    "        image_np = np.array(image)\n",
    "\n",
    "        # Récupérer les annotations associées à l'image\n",
    "        ann_ids = self.coco.getAnnIds(imgIds=img_id)\n",
    "        anns = self.coco.loadAnns(ann_ids)\n",
    "        \n",
    "        # Transformer les annotations pour rf-detr.\n",
    "        # rf-detr s'attend à une structure de cibles similaire à celle de DETR\n",
    "        boxes = []\n",
    "        labels = []\n",
    "        for ann in anns:\n",
    "            bbox = ann['bbox']  # format : [x, y, w, h]\n",
    "            # Convertir en [x_min, y_min, x_max, y_max]\n",
    "            x_min = bbox[0]\n",
    "            y_min = bbox[1]\n",
    "            x_max = x_min + bbox[2]\n",
    "            y_max = y_min + bbox[3]\n",
    "            boxes.append([x_min, y_min, x_max, y_max])\n",
    "            # Ici, vous supposez une seule classe « poisson » avec label 1 (si vous avez d'autres classes, adapter)\n",
    "            labels.append(1)\n",
    "\n",
    "        target = {\n",
    "            \"boxes\": torch.tensor(boxes, dtype=torch.float32),\n",
    "            \"labels\": torch.tensor(labels, dtype=torch.int64),\n",
    "            \"image_id\": torch.tensor([img_id])\n",
    "        }\n",
    "        \n",
    "        # Convertir en tensor (normalisation peut être ajoutée ici)\n",
    "        image_tensor = torchvision.transforms.functional.to_tensor(image_np)\n",
    "        return image_tensor, target\n",
    "\n",
    "# Créer le dataset\n",
    "dataset = CustomCOCODataset(coco_obj=coco_dataset, images_dir=images_dir)\n",
    "print(\"Taille du dataset custom :\", len(dataset))\n"
   ]
  },
  {
   "cell_type": "raw",
   "id": "65484b5b",
   "metadata": {},
   "source": [
    "# Compte tenu de la VRAM limitée (4GB), on choisit un batch size faible\n",
    "batch_size = 2\n",
    "\n",
    "data_loader = DataLoader(dataset, batch_size=batch_size, shuffle=True, collate_fn=lambda x: tuple(zip(*x)))\n",
    "\n",
    "print(\"DataLoader prêt avec batch size =\", batch_size)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d117894b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading pretrain weights\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "num_classes mismatch: pretrain weights has 90 classes, but your model has 1 classes\n",
      "reinitializing detection head with 90 classes\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Modèle chargé sur l'appareil : cuda\n"
     ]
    }
   ],
   "source": [
    "# Cellule 6 : Charger et adapter le modèle rf-detr pré-entraîné\n",
    "# -----------------------------------------------------\n",
    "# Nous allons charger le modèle rf-detr du dépôt et le fine-tuner\n",
    "# Vous pouvez soit utiliser le modèle préentraîné fourni par rf-detr (si disponible) ou initialiser un modèle de DETR et le charger\n",
    "# Vous pouvez adapter le nombre de classes. Ici, nous avons 1 classe (poisson) + 1 classe pour le fond si nécessaire.\n",
    "num_classes = 1  # par exemple, 1 classe pour \"poisson\" et 1 classe pour le \"fond\"\n",
    "\n",
    "# Chargement du modèle pré-entraîné (précisez éventuellement le checkpoint préentraîné)\n",
    "model = RFDETRBase(num_classes=num_classes, pretrained=True)\n",
    "#model.to(device)\n",
    "print(\"Modèle chargé sur l'appareil :\", device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "625a2c51",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TensorBoard logging initialized. To monitor logs, use 'tensorboard --logdir output' and open http://localhost:6006/ in browser.\n",
      "Not using distributed mode\n",
      "git:\n",
      "  sha: N/A, status: clean, branch: N/A\n",
      "\n",
      "Namespace(num_classes=1, grad_accum_steps=1, amp=True, lr=0.0001, lr_encoder=0.00015, batch_size=1, weight_decay=0.0001, epochs=15, lr_drop=100, clip_max_norm=0.1, lr_vit_layer_decay=0.8, lr_component_decay=0.7, do_benchmark=False, dropout=0, drop_path=0.0, drop_mode='standard', drop_schedule='constant', cutoff_epoch=0, pretrained_encoder=None, pretrain_weights='rf-detr-base.pth', pretrain_exclude_keys=None, pretrain_keys_modify_to_load=None, pretrained_distiller=None, encoder='dinov2_windowed_small', vit_encoder_num_layers=12, window_block_indexes=None, position_embedding='sine', out_feature_indexes=[2, 5, 8, 11], freeze_encoder=False, layer_norm=True, rms_norm=False, backbone_lora=False, force_no_pretrain=False, dec_layers=3, dim_feedforward=2048, hidden_dim=256, sa_nheads=8, ca_nheads=16, num_queries=300, group_detr=13, two_stage=True, projector_scale=['P4'], lite_refpoint_refine=True, num_select=300, dec_n_points=2, decoder_norm='LN', bbox_reparam=True, freeze_batch_norm=False, set_cost_class=2, set_cost_bbox=5, set_cost_giou=2, cls_loss_coef=1.0, bbox_loss_coef=5, giou_loss_coef=2, focal_alpha=0.25, aux_loss=True, sum_group_losses=False, use_varifocal_loss=False, use_position_supervised_loss=False, ia_bce_loss=True, dataset_file='roboflow', coco_path=None, dataset_dir='/home/mateo/Travail/Fine-tune/Dataset/Roboflow/Fish.v44i.coco', square_resize_div_64=True, output_dir='output', dont_save_weights=False, checkpoint_interval=10, seed=42, resume='', start_epoch=0, eval=False, use_ema=True, ema_decay=0.993, ema_tau=100, num_workers=2, device='cuda', world_size=1, dist_url='env://', sync_bn=True, fp16_eval=False, encoder_only=False, backbone_only=False, resolution=560, use_cls_token=False, multi_scale=True, expanded_scales=True, warmup_epochs=0, lr_scheduler='step', lr_min_factor=0.0, early_stopping=False, early_stopping_patience=10, early_stopping_min_delta=0.001, early_stopping_use_ema=False, gradient_checkpointing=False, tensorboard=True, wandb=False, project=None, run=None, class_names=[], distributed=False)\n",
      "number of params: 32174530\n",
      "[392, 448, 504, 560, 616, 672, 728, 784]\n",
      "loading annotations into memory...\n",
      "Done (t=0.00s)\n",
      "creating index...\n",
      "index created!\n",
      "[392, 448, 504, 560, 616, 672, 728, 784]\n",
      "loading annotations into memory...\n",
      "Done (t=0.00s)\n",
      "creating index...\n",
      "index created!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "fatal: not a git repository (or any of the parent directories): .git\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Get benchmark\n",
      "Start training\n",
      "Grad accum steps:  1\n",
      "Total batch size:  1\n",
      "LENGTH OF DATA LOADER: 476\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "UserWarning: torch.meshgrid: in an upcoming release, it will be required to pass the indexing argument. (Triggered internally at /pytorch/aten/src/ATen/native/TensorShape.cpp:3637.)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [0]  [  0/476]  eta: 0:09:52  lr: 0.000100  class_error: 100.00  loss: 7.7516 (7.7516)  loss_ce: 1.1628 (1.1628)  loss_bbox: 0.3373 (0.3373)  loss_giou: 0.1910 (0.1910)  loss_ce_0: 1.2347 (1.2347)  loss_bbox_0: 0.4709 (0.4709)  loss_giou_0: 0.2525 (0.2525)  loss_ce_1: 1.3176 (1.3176)  loss_bbox_1: 0.3625 (0.3625)  loss_giou_1: 0.1977 (0.1977)  loss_ce_enc: 1.0696 (1.0696)  loss_bbox_enc: 0.7610 (0.7610)  loss_giou_enc: 0.3940 (0.3940)  loss_ce_unscaled: 1.1628 (1.1628)  class_error_unscaled: 100.0000 (100.0000)  loss_bbox_unscaled: 0.0675 (0.0675)  loss_giou_unscaled: 0.0955 (0.0955)  cardinality_error_unscaled: 3899.0000 (3899.0000)  loss_ce_0_unscaled: 1.2347 (1.2347)  loss_bbox_0_unscaled: 0.0942 (0.0942)  loss_giou_0_unscaled: 0.1262 (0.1262)  cardinality_error_0_unscaled: 3899.0000 (3899.0000)  loss_ce_1_unscaled: 1.3176 (1.3176)  loss_bbox_1_unscaled: 0.0725 (0.0725)  loss_giou_1_unscaled: 0.0988 (0.0988)  cardinality_error_1_unscaled: 3899.0000 (3899.0000)  loss_ce_enc_unscaled: 1.0696 (1.0696)  loss_bbox_enc_unscaled: 0.1522 (0.1522)  loss_giou_enc_unscaled: 0.1970 (0.1970)  cardinality_error_enc_unscaled: 3899.0000 (3899.0000)  time: 1.2454  data: 0.0690  max mem: 1634\n",
      "Epoch: [0]  [ 10/476]  eta: 0:04:46  lr: 0.000100  class_error: 100.00  loss: 7.6965 (7.7272)  loss_ce: 1.2490 (1.2629)  loss_bbox: 0.2592 (0.3058)  loss_giou: 0.1910 (0.2244)  loss_ce_0: 1.2282 (1.1843)  loss_bbox_0: 0.3999 (0.4243)  loss_giou_0: 0.2406 (0.2636)  loss_ce_1: 1.2967 (1.3381)  loss_bbox_1: 0.3149 (0.3088)  loss_giou_1: 0.1925 (0.2233)  loss_ce_enc: 1.0995 (1.1034)  loss_bbox_enc: 0.5919 (0.6997)  loss_giou_enc: 0.3501 (0.3887)  loss_ce_unscaled: 1.2490 (1.2629)  class_error_unscaled: 100.0000 (100.0000)  loss_bbox_unscaled: 0.0518 (0.0612)  loss_giou_unscaled: 0.0955 (0.1122)  cardinality_error_unscaled: 3899.0000 (3897.7273)  loss_ce_0_unscaled: 1.2282 (1.1843)  loss_bbox_0_unscaled: 0.0800 (0.0849)  loss_giou_0_unscaled: 0.1203 (0.1318)  cardinality_error_0_unscaled: 3899.0000 (3897.0909)  loss_ce_1_unscaled: 1.2967 (1.3381)  loss_bbox_1_unscaled: 0.0630 (0.0618)  loss_giou_1_unscaled: 0.0963 (0.1116)  cardinality_error_1_unscaled: 3899.0000 (3895.8182)  loss_ce_enc_unscaled: 1.0995 (1.1034)  loss_bbox_enc_unscaled: 0.1184 (0.1399)  loss_giou_enc_unscaled: 0.1751 (0.1943)  cardinality_error_enc_unscaled: 3899.0000 (3892.0000)  time: 0.6147  data: 0.0087  max mem: 2117\n",
      "Epoch: [0]  [ 20/476]  eta: 0:05:09  lr: 0.000100  class_error: 0.00  loss: 7.8764 (7.8176)  loss_ce: 1.2735 (1.2791)  loss_bbox: 0.2914 (0.3750)  loss_giou: 0.1858 (0.2318)  loss_ce_0: 1.2233 (1.2245)  loss_bbox_0: 0.3999 (0.4196)  loss_giou_0: 0.2151 (0.2538)  loss_ce_1: 1.2851 (1.3132)  loss_bbox_1: 0.3149 (0.3601)  loss_giou_1: 0.1925 (0.2264)  loss_ce_enc: 1.1065 (1.1006)  loss_bbox_enc: 0.5879 (0.6608)  loss_giou_enc: 0.3287 (0.3727)  loss_ce_unscaled: 1.2735 (1.2791)  class_error_unscaled: 100.0000 (94.3223)  loss_bbox_unscaled: 0.0583 (0.0750)  loss_giou_unscaled: 0.0929 (0.1159)  cardinality_error_unscaled: 3899.0000 (3897.7143)  loss_ce_0_unscaled: 1.2233 (1.2245)  loss_bbox_0_unscaled: 0.0800 (0.0839)  loss_giou_0_unscaled: 0.1075 (0.1269)  cardinality_error_0_unscaled: 3899.0000 (3897.3810)  loss_ce_1_unscaled: 1.2851 (1.3132)  loss_bbox_1_unscaled: 0.0630 (0.0720)  loss_giou_1_unscaled: 0.0963 (0.1132)  cardinality_error_1_unscaled: 3899.0000 (3896.5714)  loss_ce_enc_unscaled: 1.1065 (1.1006)  loss_bbox_enc_unscaled: 0.1176 (0.1322)  loss_giou_enc_unscaled: 0.1644 (0.1863)  cardinality_error_enc_unscaled: 3899.0000 (3894.7143)  time: 0.6515  data: 0.0023  max mem: 2720\n",
      "Epoch: [0]  [ 30/476]  eta: 0:04:51  lr: 0.000100  class_error: 0.00  loss: 7.4634 (7.5748)  loss_ce: 1.1835 (1.2065)  loss_bbox: 0.3631 (0.3591)  loss_giou: 0.2146 (0.2270)  loss_ce_0: 1.2965 (1.2625)  loss_bbox_0: 0.3520 (0.3946)  loss_giou_0: 0.2151 (0.2421)  loss_ce_1: 1.1606 (1.2456)  loss_bbox_1: 0.3291 (0.3475)  loss_giou_1: 0.1880 (0.2193)  loss_ce_enc: 1.1303 (1.1188)  loss_bbox_enc: 0.5332 (0.5990)  loss_giou_enc: 0.2962 (0.3529)  loss_ce_unscaled: 1.1835 (1.2065)  class_error_unscaled: 46.1538 (68.6931)  loss_bbox_unscaled: 0.0726 (0.0718)  loss_giou_unscaled: 0.1073 (0.1135)  cardinality_error_unscaled: 3899.0000 (3897.9355)  loss_ce_0_unscaled: 1.2965 (1.2625)  loss_bbox_0_unscaled: 0.0704 (0.0789)  loss_giou_0_unscaled: 0.1075 (0.1211)  cardinality_error_0_unscaled: 3899.0000 (3897.6452)  loss_ce_1_unscaled: 1.1606 (1.2456)  loss_bbox_1_unscaled: 0.0658 (0.0695)  loss_giou_1_unscaled: 0.0940 (0.1096)  cardinality_error_1_unscaled: 3899.0000 (3897.1613)  loss_ce_enc_unscaled: 1.1303 (1.1188)  loss_bbox_enc_unscaled: 0.1066 (0.1198)  loss_giou_enc_unscaled: 0.1481 (0.1764)  cardinality_error_enc_unscaled: 3899.0000 (3895.9032)  time: 0.6733  data: 0.0018  max mem: 2720\n",
      "Epoch: [0]  [ 40/476]  eta: 0:04:46  lr: 0.000100  class_error: 0.00  loss: 6.7713 (7.4368)  loss_ce: 0.7353 (1.0910)  loss_bbox: 0.3411 (0.3899)  loss_giou: 0.2197 (0.2485)  loss_ce_0: 1.2901 (1.2332)  loss_bbox_0: 0.3296 (0.4030)  loss_giou_0: 0.2299 (0.2553)  loss_ce_1: 0.8379 (1.1356)  loss_bbox_1: 0.3049 (0.3775)  loss_giou_1: 0.1980 (0.2401)  loss_ce_enc: 1.1691 (1.1272)  loss_bbox_enc: 0.4395 (0.5831)  loss_giou_enc: 0.2756 (0.3524)  loss_ce_unscaled: 0.7353 (1.0910)  class_error_unscaled: 0.0000 (51.9387)  loss_bbox_unscaled: 0.0682 (0.0780)  loss_giou_unscaled: 0.1099 (0.1243)  cardinality_error_unscaled: 3899.0000 (3898.1220)  loss_ce_0_unscaled: 1.2901 (1.2332)  loss_bbox_0_unscaled: 0.0659 (0.0806)  loss_giou_0_unscaled: 0.1150 (0.1277)  cardinality_error_0_unscaled: 3899.0000 (3897.9024)  loss_ce_1_unscaled: 0.8379 (1.1356)  loss_bbox_1_unscaled: 0.0610 (0.0755)  loss_giou_1_unscaled: 0.0990 (0.1200)  cardinality_error_1_unscaled: 3899.0000 (3897.5366)  loss_ce_enc_unscaled: 1.1691 (1.1272)  loss_bbox_enc_unscaled: 0.0879 (0.1166)  loss_giou_enc_unscaled: 0.1378 (0.1762)  cardinality_error_enc_unscaled: 3899.0000 (3896.5854)  time: 0.6354  data: 0.0019  max mem: 2720\n",
      "Epoch: [0]  [ 50/476]  eta: 0:04:36  lr: 0.000100  class_error: 0.00  loss: 6.8750 (7.3980)  loss_ce: 0.6512 (1.0308)  loss_bbox: 0.3742 (0.4054)  loss_giou: 0.3148 (0.2570)  loss_ce_0: 1.0341 (1.2127)  loss_bbox_0: 0.4065 (0.4193)  loss_giou_0: 0.2568 (0.2616)  loss_ce_1: 0.7461 (1.0875)  loss_bbox_1: 0.4065 (0.3969)  loss_giou_1: 0.2976 (0.2505)  loss_ce_enc: 1.1730 (1.1356)  loss_bbox_enc: 0.4578 (0.5866)  loss_giou_enc: 0.3350 (0.3541)  loss_ce_unscaled: 0.6512 (1.0308)  class_error_unscaled: 0.0000 (41.7547)  loss_bbox_unscaled: 0.0748 (0.0811)  loss_giou_unscaled: 0.1574 (0.1285)  cardinality_error_unscaled: 3899.0000 (3898.1765)  loss_ce_0_unscaled: 1.0341 (1.2127)  loss_bbox_0_unscaled: 0.0813 (0.0839)  loss_giou_0_unscaled: 0.1284 (0.1308)  cardinality_error_0_unscaled: 3899.0000 (3898.0000)  loss_ce_1_unscaled: 0.7461 (1.0875)  loss_bbox_1_unscaled: 0.0813 (0.0794)  loss_giou_1_unscaled: 0.1488 (0.1252)  cardinality_error_1_unscaled: 3899.0000 (3897.7059)  loss_ce_enc_unscaled: 1.1730 (1.1356)  loss_bbox_enc_unscaled: 0.0916 (0.1173)  loss_giou_enc_unscaled: 0.1675 (0.1770)  cardinality_error_enc_unscaled: 3899.0000 (3896.9412)  time: 0.6424  data: 0.0018  max mem: 2721\n",
      "Epoch: [0]  [ 60/476]  eta: 0:04:29  lr: 0.000100  class_error: 0.00  loss: 7.3569 (7.5328)  loss_ce: 0.6642 (0.9908)  loss_bbox: 0.4898 (0.4607)  loss_giou: 0.3203 (0.2780)  loss_ce_0: 1.0109 (1.1907)  loss_bbox_0: 0.5952 (0.4655)  loss_giou_0: 0.3280 (0.2776)  loss_ce_1: 0.7718 (1.0495)  loss_bbox_1: 0.5821 (0.4558)  loss_giou_1: 0.3133 (0.2706)  loss_ce_enc: 1.1730 (1.1394)  loss_bbox_enc: 0.5847 (0.5947)  loss_giou_enc: 0.3709 (0.3596)  loss_ce_unscaled: 0.6642 (0.9908)  class_error_unscaled: 0.0000 (35.0357)  loss_bbox_unscaled: 0.0980 (0.0921)  loss_giou_unscaled: 0.1602 (0.1390)  cardinality_error_unscaled: 3899.0000 (3898.2623)  loss_ce_0_unscaled: 1.0109 (1.1907)  loss_bbox_0_unscaled: 0.1190 (0.0931)  loss_giou_0_unscaled: 0.1640 (0.1388)  cardinality_error_0_unscaled: 3899.0000 (3898.1148)  loss_ce_1_unscaled: 0.7718 (1.0495)  loss_bbox_1_unscaled: 0.1164 (0.0912)  loss_giou_1_unscaled: 0.1566 (0.1353)  cardinality_error_1_unscaled: 3899.0000 (3897.8689)  loss_ce_enc_unscaled: 1.1730 (1.1394)  loss_bbox_enc_unscaled: 0.1169 (0.1189)  loss_giou_enc_unscaled: 0.1854 (0.1798)  cardinality_error_enc_unscaled: 3899.0000 (3897.2295)  time: 0.6242  data: 0.0017  max mem: 2721\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [0]  [ 70/476]  eta: 0:04:23  lr: 0.000100  class_error: 0.00  loss: 7.4042 (7.5823)  loss_ce: 0.6889 (0.9733)  loss_bbox: 0.5445 (0.4789)  loss_giou: 0.3422 (0.2956)  loss_ce_0: 0.9684 (1.1564)  loss_bbox_0: 0.6201 (0.4877)  loss_giou_0: 0.3497 (0.2974)  loss_ce_1: 0.7718 (1.0169)  loss_bbox_1: 0.6091 (0.4853)  loss_giou_1: 0.3400 (0.2909)  loss_ce_enc: 1.1748 (1.1458)  loss_bbox_enc: 0.5357 (0.5881)  loss_giou_enc: 0.3515 (0.3659)  loss_ce_unscaled: 0.6889 (0.9733)  class_error_unscaled: 0.0000 (30.2095)  loss_bbox_unscaled: 0.1089 (0.0958)  loss_giou_unscaled: 0.1711 (0.1478)  cardinality_error_unscaled: 3899.0000 (3898.1549)  loss_ce_0_unscaled: 0.9684 (1.1564)  loss_bbox_0_unscaled: 0.1240 (0.0975)  loss_giou_0_unscaled: 0.1748 (0.1487)  cardinality_error_0_unscaled: 3899.0000 (3898.0282)  loss_ce_1_unscaled: 0.7718 (1.0169)  loss_bbox_1_unscaled: 0.1218 (0.0971)  loss_giou_1_unscaled: 0.1700 (0.1455)  cardinality_error_1_unscaled: 3899.0000 (3897.8169)  loss_ce_enc_unscaled: 1.1748 (1.1458)  loss_bbox_enc_unscaled: 0.1071 (0.1176)  loss_giou_enc_unscaled: 0.1757 (0.1830)  cardinality_error_enc_unscaled: 3899.0000 (3897.2676)  time: 0.6489  data: 0.0017  max mem: 2722\n"
     ]
    }
   ],
   "source": [
    "model.train(dataset_dir=os.path.join(data_dir, \"Fish.v44i.coco\"), epochs=15, batch_size=1, grad_accum_steps=1, lr=1e-4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c262338",
   "metadata": {},
   "outputs": [],
   "source": [
    "Image.open(\"output/metrics_plot.png\")\n",
    "# Affiche un résumé rapide du modèle\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5d8b3a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import supervision as sv\n",
    "\n",
    "ds = sv.DetectionDataset.from_coco(\n",
    "    images_directory_path=os.path.join(data_dir, \"Fish.v44i.coco\", \"test\"),\n",
    "    annotations_path=os.path.join(data_dir, \"Fish.v44i.coco\", \"test\",\"_annotations.coco.json\")\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36687a79",
   "metadata": {},
   "outputs": [],
   "source": [
    "path, image, annotations = ds[38]\n",
    "image = Image.open(path)\n",
    "\n",
    "detections = model.predict(image, threshold=0.3)\n",
    "\n",
    "text_scale = sv.calculate_optimal_text_scale(resolution_wh=image.size)\n",
    "thickness = sv.calculate_optimal_line_thickness(resolution_wh=image.size)\n",
    "\n",
    "bbox_annotator = sv.BoxAnnotator(thickness=thickness)\n",
    "label_annotator = sv.LabelAnnotator(\n",
    "    text_color=sv.Color.BLACK,\n",
    "    text_scale=text_scale,\n",
    "    text_thickness=thickness,\n",
    "    smart_position=True)\n",
    "\n",
    "annotations_labels = [\n",
    "    f\"{ds.classes[class_id]}\"\n",
    "    for class_id\n",
    "    in annotations.class_id\n",
    "]\n",
    "\n",
    "detections_labels = [\n",
    "    f\"{ds.classes[class_id]} {confidence:.2f}\"\n",
    "    for class_id, confidence\n",
    "    in zip(detections.class_id, detections.confidence)\n",
    "]\n",
    "\n",
    "annotation_image = image.copy()\n",
    "annotation_image = bbox_annotator.annotate(annotation_image, annotations)\n",
    "annotation_image = label_annotator.annotate(annotation_image, annotations, annotations_labels)\n",
    "\n",
    "detections_image = image.copy()\n",
    "detections_image = bbox_annotator.annotate(detections_image, detections)\n",
    "detections_image = label_annotator.annotate(detections_image, detections, detections_labels)\n",
    "\n",
    "sv.plot_images_grid(images=[annotation_image, detections_image], grid_size=(1, 2), titles=[\"Annotation\", \"Detection\"])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (rf-detr)",
   "language": "python",
   "name": "rfdetr"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
